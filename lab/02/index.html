<!DOCTYPE HTML>
<html lang="zh-Hans" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>实验 2：使用 FMM、BMM 算法分词 - Learn NLP</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../highlight.css">
        <link rel="stylesheet" href="../../tomorrow-night.css">
        <link rel="stylesheet" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../theme/css/custom-style.css">
        <link rel="stylesheet" href="../../theme/css/custom-font.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../index.html">首页</a></li><li class="chapter-item expanded "><a href="../../lab/index.html"><strong aria-hidden="true">1.</strong> 实验</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../lab/01/index.html"><strong aria-hidden="true">1.1.</strong> 实验 1：互联网语料获取</a></li><li class="chapter-item expanded "><a href="../../lab/02/index.html" class="active"><strong aria-hidden="true">1.2.</strong> 实验 2：使用 FMM、BMM 算法分词</a></li><li class="chapter-item expanded "><a href="../../lab/03/index.html"><strong aria-hidden="true">1.3.</strong> 实验 3：使用 2 元文法消除歧义</a></li><li class="chapter-item expanded "><a href="../../lab/04/index.html"><strong aria-hidden="true">1.4.</strong> 实验 4：基于 HMM 的文本分词</a></li><li class="chapter-item expanded "><a href="../../lab/misc/report-template.html"><strong aria-hidden="true">1.5.</strong> 实验报告模板</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Learn NLP</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/tsagaanbar/learn-NLP" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/tsagaanbar/learn-NLP/edit/main/src/lab/02/README.md" title="Suggest an edit" aria-label="Suggest an edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="自然语言处理"><a class="header" href="#自然语言处理">自然语言处理</a></h1>
<p style="text-align: center;">实验二</p>
<img src="../misc/logo.svg" alt="School Logo" style="max-width: 30%;">
<table>
    <style>
        td.distrib { text-justify: distribute; text-align-last: justify; text-align: justify; text-align: center; }
        td.center { text-align: center; }
    </style>
    <tbody>
        <tr><td class="distrib">学院</td><td class="center">信息工程学院</td></tr>
        <tr><td class="distrib">指导教师</td><td class="center">孙媛</td></tr>
        <tr><td class="distrib">班级</td><td class="center">19 级计算机科学与技术 1 班</td></tr>
        <tr><td class="distrib">学生姓名</td><td class="center">John Doe</td></tr>
        <tr><td class="distrib">学号</td><td class="center">19000000</td></tr>
    </tbody>
</table>
<p style="text-align: center;">日期： 2021 年 10 月 21 日</p>
<div style="page-break-after: always;"></div>
<h2 id="目录"><a class="header" href="#目录">目录</a></h2>
<ol style="list-style-type: none;">
    <li><a href="#自然语言处理实验二">自然语言处理：实验二</a>
        <ol style="list-style-type: none;">
            <li><a href="#一实验内容">一、实验内容</a></li>
            <li><a href="#二实验原理">二、实验原理</a>
                <ol style="list-style-type: none;">
                    <li><a href="#基于词典的切分方法">基于词典的切分方法</a></li>
                    <li><a href="#jieba-分词库">jieba 分词库</a></li>
                    <li><a href="#扩充词典">扩充词典</a></li>
                    <li><a href="#准确率精确率召回率及-F-值">准确率、精确率、召回率及 F 值</a></li>
                    <li><a href="#分词结果的评估">分词结果的评估</a></li>
                </ol>
            </li>
            <li><a href="#三整体框架">三、整体框架</a></li>
            <li><a href="#四主要程序模块">四、主要程序模块</a>
                <ol style="list-style-type: none;">
                    <li><a href="#fmmbmm">FMM、BMM</a></li>
                    <li><a href="#评估分词结果">评估分词结果</a></li>
                    <li><a href="#多文件处理">多文件处理</a></li>
                    <li><a href="#交互式分词程序">交互式分词程序</a></li>
                    <li><a href="#训练及测试">训练及测试</a></li>
                    <li><a href="#trie-模块">Trie 模块</a></li>
                </ol>
            </li>
            <li><a href="#五实验结果">五、实验结果</a>
                <ol style="list-style-type: none;">
                    <li><a href="#基本实验结果">基本实验结果</a></li>
                    <li><a href="#词典容量的影响">词典容量的影响</a></li>
                </ol>
            </li>
            <li><a href="#六总结">六、总结</a>
                <ol style="list-style-type: none;">
                    <li><a href="#关于如何计算正确匹配数">关于如何计算正确匹配数</a></li>
                    <li><a href="#参考资料">参考资料</a></li>
                </ol>
            </li>
        </ol>
    </li>
</ol>
<div style="page-break-after: always;"></div>
<h2 id="一实验内容"><a class="header" href="#一实验内容">一、实验内容</a></h2>
<p>实验内容：</p>
<ul>
<li>对语料库的文本进行分词并存储。</li>
<li>分别采用正向最大匹配算法、逆向最大匹配算法进行分词。以 <a href="https://github.com/fxsjy/jieba">jieba</a> 分词的分词结果作为标准语料，计算P、R、F值。</li>
</ul>
<h2 id="二实验原理"><a class="header" href="#二实验原理">二、实验原理</a></h2>
<h3 id="基于词典的切分方法"><a class="header" href="#基于词典的切分方法">基于词典的切分方法</a></h3>
<p>句子 \(S = c_1 c_2 \cdots c_n\)：句子由若干字符 \(c\) 组成。</p>
<p>假设词 \(w_i = c_1 c_2 \cdots c_m\)，其中 \(m\) 为词典中最长词的字数。</p>
<p>当前的分词算法主要分为两类——基于词典的规则匹配方法和基于统计的机器学习方法。</p>
<p>基于词典的分词算法，本质上就是字符串匹配。将待匹配的字符串基于一定的算法策略，和一个足够大的词典进行字符串匹配，如果匹配命中，则可以分词。根据不同的匹配策略，又分为正向最大匹配法，逆向最大匹配法，双向匹配分词，全切分路径选择等。</p>
<p>最大匹配法（Maximum Matching, MM）主要分为三种：</p>
<ul>
<li>正向最大匹配算法（Forward MM, FMM）：从左到右对语句进行匹配，匹配的词越长越好。这种方式切分会有歧义问题出现。</li>
<li>逆向最大匹配算法（Backward MM, BMM）：从右到左对语句进行匹配，同样也是匹配的词越长越好。这种方式同样也会有歧义问题。</li>
<li>双向最大匹配算法（Bi-directional MM）：则同时采用正向最大匹配和逆向最大匹配，选择二者分词结果中词数较少者。但这种方式同样会产生歧义问题。由此可见，词数少也不一定划分就正确。</li>
</ul>
<p><strong>全切分路径选择</strong>，将所有可能的切分结果全部列出来，从中选择最佳的切分路径。分为两种选择方法：</p>
<ol>
<li>n 最短路径方法。将所有的切分结果组成有向无环图，切词结果作为节点，词和词之间的边赋予权重，找到权重和最小的路径即为最终结果。比如可以通过词频作为权重，找到一条总词频最大的路径即可认为是最佳路径。</li>
<li>n 元语法模型。同样采用 n 最短路径，只不过路径构成时会考虑词的上下文关系。一元表示考虑词的前后一个词，二元则表示考虑词的前后两个词。然后根据语料库的统计结果，找到概率最大的路径。</li>
</ol>
<p>此次实验默认使用的是实验指导书中提供的词典，分别采用正向最大匹配算法、逆向最大匹配算法进行分词。</p>
<h3 id="jieba-分词库"><a class="header" href="#jieba-分词库">jieba 分词库</a></h3>
<p><a href="https://github.com/fxsjy/jieba">jieba</a> 支持三种分词模式:</p>
<ol>
<li>精确分词，试图将句子最精确的切开，适合文本分析</li>
<li>全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义</li>
<li>搜索引擎模式，在精确模式基础上，对长词进行再次切分，提高recall，适合于搜索引擎。</li>
</ol>
<h3 id="扩充词典"><a class="header" href="#扩充词典">扩充词典</a></h3>
<p>将过程中遇到的“新词”添加进词典，可以提高文本识别的准确率。</p>
<h3 id="准确率精确率召回率及-f-值"><a class="header" href="#准确率精确率召回率及-f-值">准确率、精确率、召回率及 F 值</a></h3>
<p>机器学习中的分类评估包含有以下这么几个概念。</p>
<p><strong>准确率</strong>（Accuracy），即正确分类的数量占总的数量的比值，是一个用来衡量分类器预测结果与真实结果差异的一个指标，越接近于 1 说明分类结果越准确。</p>
<p>二分类的结果有以下几种可能性：</p>
<ul>
<li>True Positive（TP）：表示将正样本预测为正样本，即预测正确；</li>
<li>False Positive（FP）：表示将负样本预测为正样本，即预测错误；</li>
<li>False Negative（FN）：表示将正样本预测为负样本，即预测错误；</li>
<li>True Negative（TN）：表示将负样本预测为负样本，即预测正确；</li>
</ul>
<p><strong>精确率</strong>（Precision）计算的是预测对的正样本在整个预测为正样本中的比重，而<strong>召回率</strong>（Recall）计算的是预测对的正样本在整个真实正样本中的比重。因此，一般来说，召回率越高，意味着模型找寻正样本的能力越强。</p>
<p>准确率、精确率、召回率的计算公式如下：</p>
<p>\[
\begin{align}
\text{Accuracy} = &amp; \frac{TP+TN}{TP+FP+FN+TN} \\
\text{Precision} = &amp; \frac{TP}{TP+FP} \\
\text{Recall} = &amp; \frac{TP}{TP+FN} \\
\end{align}
\]</p>
<p>值得注意的是，在实际任务中，并不明确哪一类是正样本或哪一类是负样本，所以对于每个类别，都可以计算其各项指标。</p>
<p>实际评估一个系统时，应同时考虑 P 和 R，但同时要比较两个数值，很难做到一目了然。所以常采用综合两个值进行评价的办法，综合指标 F 值就是其中一种。计算公式如下：</p>
<p>\[
\text{F-score} = (1+\beta^2)\frac{P \times R}{\beta^2 \times P + R}
\]</p>
<p>其中，\(\beta\) 决定对 P 侧重还是对 R 侧重，通常设定为 1、2 或 \(\frac 1 2\)。\(\beta\) 取值为 1，即对二者一样重视，这时的 F-score 称为 \(F_1\) 值。</p>
<h3 id="分词结果的评估"><a class="header" href="#分词结果的评估">分词结果的评估</a></h3>
<p>机器学习中二分类的评估标准，无法直接应用于分词。</p>
<p>在对汉语分词性能进行评估时，采用了常用的３个评测指标：准确率（P）、召回率（R）、综合指标 F 值（F）。准确率表示在切分的全部词语中，正确的所占的比值。召回率指在所有切分词语中（包括切分的和不应该忽略的），正确切分的词语所占的比值。准确率描述系统切分的词语中，正确的占多少。召回率表示应该得到的词语中，系统正确切分出了多少。计算公式如下：</p>
<p>\[
P = \frac{\text{准确切分的词语数}}{\text{切分出的所有词语数}}
\]</p>
<p>\[
R = \frac{\text{准确切分的词语数}}{\text{应该切分的词语数}}
\]</p>
<p>若一字符串的分词结果为一系列单词，设每个单词按照其在文中的起止位置可记作区间 \([i,j]\)（\(0\leq i \leq j \leq n\)），那么标准答案对应的所有区间就可以构成一集合 \(A\)，作为正类，其他的区间则作为负类;同理，根据分词结果，可以得到集合 \(B\)。</p>
<p>\[TP \cup FN = A \]</p>
<p>\[TP \cup FP = B \]</p>
<p>\[A \cap B = TP \]</p>
<p>则对于分词结果，P、R 的计算公式：</p>
<p>\[\text{Precision} = \frac{\vert A\cap B\vert}{\vert B \vert}\]</p>
<p>\[\text{Recall} = \frac{\vert A\cap B\vert}{\vert A \vert}\]</p>
<h2 id="三整体框架"><a class="header" href="#三整体框架">三、整体框架</a></h2>
<p>主要分为功能模块和使用调用。详见主要程序模块。</p>
<ul>
<li>FMM、BMM 分词：<a href="./FMM_BMM.py">FMM_BMM.py</a>、<a href="./FMM_BMM_trie.py">FMM_BMM_trie.py</a></li>
<li>多文件处理：<a href="./file_processing.py">file_processing.py</a></li>
</ul>
<h2 id="四主要程序模块"><a class="header" href="#四主要程序模块">四、主要程序模块</a></h2>
<h3 id="fmmbmm"><a class="header" href="#fmmbmm">FMM、BMM</a></h3>
<p>从文件中加载字典到 Python 内建的 <code>set</code> 类型，以及进行最大正向匹配（FMM）、最大逆向匹配（BMM）的功能：</p>
<p><a href="./FMM_BMM.py">FMM_BMM.py</a> </p>
<pre><code class="language-python">def load_wordlist(filename):
    wordlist = set()
    maxlen = 0
    cnt = 0
    with open(filename) as f:
        # 判断文件首行是否为指定的 'magic code'
        if f.readline().strip() == '@Lexicon':
            for line in f:
                word = line.split()[1]
                maxlen = max(maxlen, len(word))
                wordlist.add(word)
                cnt += 1
    # maxlen 为字典中最长词的长度
    return wordlist, cnt, maxlen


def add_words(wordlist, cnt, maxlen, new_words):
    for w in new_words:
        w = w.strip()
        if len(w) == 0:
            continue
        if w.startswith(&quot;-&quot;) or w.startswith(&quot;.&quot;):
            # 排除可能的特殊符号串，如“----------”
            continue
        if w.isdigit():
            # 同理，排除掉特殊情况，使得字典更具有普遍性
            continue
        cnt += 1
        maxlen = max(len(w), maxlen)
        wordlist.add(w)
    return wordlist, cnt, maxlen


def save_wordlist(filename, wordlist):
    with open(filename, &quot;w&quot;, encoding='utf-8') as out:
        out.write('@Lexicon\n')
        cnt = 0
        for w in wordlist:
            cnt += 1
            out.write(str(cnt) + ' ' + w + '\n')


def FMM(sentence, wordlist, maxlen):
    maxlen = max(1, maxlen)
    tokens = []
    i = 0
    while i &lt; len(sentence):
        n = len(sentence) - i # 未被切分的字串长度
        m = min(maxlen, n)
        w = sentence[i:i+m]
        while len(w) &gt; 1:
            if w in wordlist:
                break
            else:
                w = w[0:-1]
        tokens.append(w)
        i += len(w)
    return tokens


def BMM(sentence, wordlist, maxlen):
    maxlen = max(1, maxlen)
    tokens = []
    i = len(sentence)
    while i &gt;= 1:
        n = i # 未被切分的字串长度
        m = min(maxlen, n)
        w = sentence[i-m:i]
        while len(w) &gt; 1:
            if w in wordlist:
                break
            else:
                w = w[1:]
        tokens.append(w)
        i -= len(w)
    tokens.reverse()
    return tokens

</code></pre>
<h3 id="评估分词结果"><a class="header" href="#评估分词结果">评估分词结果</a></h3>
<p>评估分词结果，以及计算 P、R、F 值：</p>
<p><a href="./calc.py">calc.py</a></p>
<pre><code class="language-python">special_characters = set(list(
    &quot;()[]+-*/&lt;&gt;|\\;:\&quot;\'\,.?!@#$%^&amp;~`\{\}（）【】《》，。？“”‘’；：——「」『』〔〕&quot;
))

#包括部分特殊字符，在进行分词比对时将特殊字符排除，以免对结果产生一定的影响

def calc_hits(truth, result):
    # 传入分词得到的结果（列表），以及“正确分词”结果
    cut_truth = [item for item in truth if item not in special_characters]
    cut_result = [item for item in result if item not in special_characters]
    i = 0       # 指向 truth 中的 token
    j = 0       # 指向 result 中的 token
    l1 = 0      # i 所指向词串，对应在原句中的长度
    l2 = 0      # j 所指向词串，对应在原句中的长度
    hits = 0
    missmatches = set()
    while i &lt; len(cut_truth) and j &lt; len(cut_result):
        if l1 &lt; l2:
            l1 += len(cut_truth[i])
            i += 1
        elif l1 &gt; l2:
            l2 += len(cut_result[j])
            j += 1
        else: 
            if cut_truth[i] == cut_result[j]:
                hits += 1
            else:
                # 记录未匹配到的“新词”
                missmatches.add(cut_truth[i])
            l1 += len(cut_truth[i])
            i += 1
            l2 += len(cut_result[j])
            j += 1
    
    return hits, len(truth), len(result), missmatches


def calc_PRF(hits, truth_len, result_len):
    P = hits / result_len       # precision
    R = hits / truth_len        # recall
    F = (2 * P * R) / (P + R)   # F_1
    return P, R, F
</code></pre>
<h3 id="多文件处理"><a class="header" href="#多文件处理">多文件处理</a></h3>
<p><a href="./file_processing.py">file_processing.py</a></p>
<pre><code class="language-python"># -*-coding:UTF-8 -*-
import os
import time

def cut_txt(file_name, cut_methods_list):
    &quot;&quot;&quot;
    单个TXT文档处理，可以接收多个方法，
    :param file_name:   要处理的文件名
    :param cut_methods_list: 类似如下的列表 [(&quot;name&quot;, method), ...]
    :return: 存储分词结果的字典，以方法名作为键名
    &quot;&quot;&quot;
    results = dict()
    for pair in cut_methods_list:
        results[pair[0]] = []
    
    elapsed_time = 0
    with open(file_name, &quot;r&quot;, encoding='utf-8') as f:
        for line in f:
            for pair in cut_methods_list:
                foo_name = pair[0]
                foo = pair[1]
                start = time.time()
                results[foo_name] += foo(line.strip())
                end = time.time()
                elapsed_time += (end - start)

    return results, elapsed_time


def write_results(file_name, results, delimiter='/', output_dir_prefix=''):
    &quot;&quot;&quot;
    将分词结果写出到文件
    &quot;&quot;&quot;
    for foo_name in results.keys():
        out_file_name = os.path.join(
            output_dir_prefix, 
            file_name + &quot;.&quot; + foo_name + &quot;.segmented&quot;)
        with open(out_file_name, 'w', encoding='utf-8') as out:
            out.write(
                delimiter.join(results[foo_name]))


def process_path(path, cut_methods_list):
    &quot;&quot;&quot;
    对目录下的所有文件进行处理
    :param path: 目录名
    :param cut_methods_list: 要采用的分词方法的列表
    :return: 文件名及对应结果的生成器
    &quot;&quot;&quot;
    ignore = ['href', '简介', 'segmented', '#']
    for root, subdirs, files in os.walk(path):
        for f in files:
            file_name = os.path.join(root, f)

            # 如果文件名含有某些特征，跳过
            should_pass = False
            for kw in ignore:
                if file_name.find(kw) != -1:
                    should_pass = True
                    break
            if should_pass: continue

            # 当前正在处理的文件
            print(' - Processing:', file_name)
            
            # 获得各方法的分词结果：results
            
            results, t = cut_txt(file_name, cut_methods_list)

            yield file_name, results, t 

</code></pre>
<p>在 <code>简介.txt</code> 和 <code>href.txt</code> 中含有 URL 链接，对其进行处理会在一定程度上影响到分词的结果，亦会产生较多无意义的“新词”，故不对其进行处理。</p>
<h3 id="交互式分词程序"><a class="header" href="#交互式分词程序">交互式分词程序</a></h3>
<p><a href="./demo.py">demo.py</a></p>
<pre><code class="language-python">from FMM_BMM_trie import *
from calc import *

import jieba

wordlist, cnt, maxlen = load_wordlist('wordlist.dic')

while True:
    s = input().strip()
    if s == '#':
        break
    ground_truth = jieba.lcut(s)  # 'ground truth'
    fmm = FMM(s, wordlist, maxlen)
    bmm = BMM(s, wordlist, maxlen)

    print('jieba:     ', &quot;/&quot;.join(ground_truth))

    print('-' * 20)

    print('FMM:       ', &quot;/&quot;.join(fmm))
    hits, len_truth, len_result, _ = calc_hits(ground_truth, fmm)
    P, R, F = calc_PRF(hits, len_truth, len_result)
    print('Hits:      ', hits)
    print('Precision: ', P)
    print('Recall:    ', R)
    print('F:         ', F)

    print('-' * 20)

    print('BMM:       ', &quot;/&quot;.join(bmm))
    hits, len_truth, len_result, _ = calc_hits(ground_truth, bmm)
    P, R, F = calc_PRF(hits, len_truth, len_result)
    print('Hits:      ', hits)
    print('Precision: ', P)
    print('Recall:    ', R)
    print('F:         ', F)
</code></pre>
<h3 id="训练及测试"><a class="header" href="#训练及测试">训练及测试</a></h3>
<p>打印统计信息帮助函数：</p>
<p><a href="./print_helper.py">print_helper.py</a></p>
<pre><code class="language-python">from numpy.core.numeric import NaN
from calc import calc_PRF

def print_stat(foo_name, total_truth_cnt, total_result_cnt, total_hits):

    print('{} 分词结果：'.format(foo_name))
    print(&quot;{:&lt;8} 分词总共的数目：{}&quot;.format(&quot;jieba&quot;, total_truth_cnt))
    print(&quot;{:&lt;8} 分词总共的数目：{}&quot;.format(foo_name, total_result_cnt))
    print(&quot;{:&lt;8} 分词正确的数目：{}&quot;.format(foo_name, total_hits))

    P, R, F = NaN, NaN, NaN

    if total_result_cnt != 0 and total_truth_cnt != 0:
        P, R, F = calc_PRF(total_hits, total_truth_cnt, total_result_cnt)

    print(&quot;准确率（P）：{:.5f} %&quot;.format(100 * P))
    print(&quot;回归率（R）：{:.5f} %&quot;.format(100 * R))
    print(&quot;F 值为：{}&quot;.format(F))
</code></pre>
<p>对文档进行分词尝试，使用 jieba 分词以及 FMM、BMM 两种最大匹配算法进行分词，并评估 FMM、BMM 分词的结果。</p>
<p>同时，根据与 jieba 分词结果的差异得到“新词”并添加进词典。最后，将得到的新词典写出到 <code>new_wordlist.dic</code> 文件：</p>
<p><a href="./train.py">train.py</a></p>
<pre><code class="language-python">import jieba
from FMM_BMM import *   # 根据路径不同修改
from calc import *
from file_processing import * 

trainning_file_path = [&quot;训练语料&quot;]

wordlist, cnt, maxlen = load_wordlist('wordlist.dic')

FMM_cut = lambda line : FMM(line, wordlist, maxlen)
BMM_cut = lambda line : BMM(line, wordlist, maxlen)

methods = [
    ('jieba', jieba.lcut),
    ('FMM', FMM_cut),
    ('BMM', BMM_cut)
]

tot_hits = {'FMM': 0, 'BMM': 0}         # 统计 FMM/BMM 分词结果正确的个数
tot_result_cnt = {'FMM': 0, 'BMM': 0}   # 统计 FMM/BMM 分词结果的个数
tot_truth_cnt = 0                       # 统计 jieba 分词结果的个数

new_words = []

tot_elapsed_time = 0

for path in trainning_file_path:
    # 对某一目录下结果进行处理
    for filename, results, elapsed_time in process_path(path, methods):
        
        if len(results) == 0: continue
        
        truth = results['jieba']
        fmm = results['FMM']
        bmm = results['BMM']

        hits, len_truth, len_result, missmatches = calc_hits(truth, fmm)
        tot_hits['FMM'] += hits
        tot_result_cnt['FMM'] += len_result
        new_words += missmatches

        hits, len_truth, len_result, missmatches = calc_hits(truth, bmm)
        tot_hits['BMM'] += hits
        tot_result_cnt['BMM'] += len_result
        new_words += missmatches
        
        tot_truth_cnt += len_truth

        tot_elapsed_time += elapsed_time
        
        # 对同一个文档进行的 FMM 和 BMM 处理，虽然存在差异
        # 但大部分分词结果相同，故只将 FMM 分词结果存储

        # write_results(filename, {'FMM': results[&quot;FMM&quot;]}, '/')

print('总耗时：{} s'.format(tot_elapsed_time))

import print_helper as helper

# 打印统计信息

helper.print_stat('FMM', tot_truth_cnt, tot_result_cnt['FMM'], tot_hits['FMM'])
helper.print_stat('BMM', tot_truth_cnt, tot_result_cnt['BMM'], tot_hits['BMM'])

# 将新词添加入词典
wordlist, cnt, maxlen = add_words(wordlist, cnt, maxlen, new_words)

save_wordlist('new_wordlist.dic', wordlist)
</code></pre>
<p>使用训练后得到的词典，进行 FMM、BMM 分词，并对结果评估（亦以 jieba 分词的结果为标准答案）：</p>
<p><a href="./test.py">test.py</a></p>
<pre><code class="language-python">import jieba
from FMM_BMM import *   # 根据路径不同修改
from calc import *
from file_processing import * 

wordlist, cnt, maxlen = load_wordlist('new_wordlist.Dic')

FMM_cut = lambda line : FMM(line, wordlist, maxlen)
BMM_cut = lambda line : BMM(line, wordlist, maxlen)

methods = [
    ('jieba', jieba.lcut),
    ('FMM', FMM_cut),
    ('BMM', BMM_cut)
]

test_file_path = [&quot;测试语料&quot;]

tot_hits = {'FMM': 0, 'BMM': 0}
tot_result_cnt = {'FMM': 0, 'BMM': 0}
tot_truth_cnt = 0

tot_elapsed_time = 0

for path in test_file_path:
    # 对某一目录下结果进行处理
    for filename, results, elapsed_time in process_path(path, methods):
        
        if len(results) == 0: continue

        truth = results['jieba']
        fmm = results['FMM']
        bmm = results['BMM']

        hits, len_truth, len_result, _ = calc_hits(truth, fmm)
        tot_hits['FMM'] += hits
        tot_result_cnt['FMM'] += len_result

        hits, len_truth, len_result, _ = calc_hits(truth, bmm)
        tot_hits['BMM'] += hits
        tot_result_cnt['BMM'] += len_result
        
        tot_truth_cnt += len_truth

        tot_elapsed_time += elapsed_time


print(&quot;------------------训练后---------------------&quot;)

print('总耗时：{} s'.format(tot_elapsed_time))

import print_helper as helper

# 打印统计信息

helper.print_stat('FMM', tot_truth_cnt, tot_result_cnt['FMM'], tot_hits['FMM'])
helper.print_stat('BMM', tot_truth_cnt, tot_result_cnt['BMM'], tot_hits['BMM'])
</code></pre>
<h3 id="trie-模块"><a class="header" href="#trie-模块">Trie 模块</a></h3>
<p>考虑到可能会涉及大量词语的存储与检索，尝试使用将词库载入并存储于自己实现的 Trie 字典树结构中。</p>
<blockquote>
<p>实际测试时，使用该 Trie 实现的效率不及使用 Python 内建的集合类型。</p>
</blockquote>
<p>为了简便起见，使用 Python 中的字典结构模拟节点对象。</p>
<p><a href="./my_trie.py">my_trie.py</a></p>
<pre><code class="language-python">def insert(node, s):
    current = node
    for c in s:
        if c not in current:
            current[c] = dict()
        current = current[c]
    current['end'] = True


def find(node, s):
    for c in s:
        if c not in node:
            return False
        node = node[c]
    return 'end' in node


def traverse(node, s=''):
    for key in node.keys():
        if key == 'end':
            continue
        s += key
        yield from traverse(node[key], s)
        s = s[:-1]
    if 'end' in node:
        yield s

</code></pre>
<p>使用示例：<a href="./trie_test.py">trie_test.py</a></p>
<p>使用 trie 进行词典检索的 FMM、BMM：<a href="./FMM_BMM_trie.py">FMM_BMM_trie.py</a>。</p>
<p>实际效率情况对比：</p>
<ul>
<li>使用自己编写的 Trie 模块：
<ul>
<li><a href="assets/trie-before-train.png">trie-before-train.png</a></li>
<li><a href="assets/trie-after-train.png">trie-after-train.png</a></li>
</ul>
</li>
<li>使用 Python 内建的 <code>set</code> 类型：
<ul>
<li><a href="assets/python-set-before-train.png">python-set-before-train.png</a></li>
<li><a href="assets/python-set-after-train.png">python-set-after-train.png</a></li>
</ul>
</li>
</ul>
<h2 id="五实验结果"><a class="header" href="#五实验结果">五、实验结果</a></h2>
<h3 id="基本实验结果"><a class="header" href="#基本实验结果">基本实验结果</a></h3>
<p>实验采用的语料来自于知乎问答平台上针对“大学”的回答。初步实验材料的大小如下：</p>
<figure>
<img src="./assets/train-material.png" 
    alt="用于训练的材料：2789 个文件，2.63 MB" 
    title="用于训练的材料大小"
    style="max-width: 50%;">
<figcaption>图 1：用于训练的材料（2789 个文件，2.63 MB）</figcaption>
</figure>
<figure>
<img src="./assets/test-material.png" 
    alt="用于测试的材料：1747 个文件，1.61 MBB" 
    title="用于测试的材料大小"
    style="max-width: 50%;">
<figcaption>图 2：用于测试的材料（1747 个文件，1.61 MB）</figcaption>
</figure>
<p>首先使用默认字典（53143 词），对训练材料进行分词，并对结果评估，最后产生新的字典。（使用 Python 内建的 set 作为存储词典的结构）</p>
<p>代码：<a href="./train.py">train.py</a></p>
<figure>
<p><img src="./assets/python-set-before-train.png" alt="使用默认词典的情况下，FMM、BMM 法分词的结果" /></p>
<figcaption>图 3：使用默认词典的情况下，FMM、BMM 法分词的结果</figcaption>
</figure>
<pre><code>FMM 分词结果：
jieba    分词总共的数目：572064
FMM      分词总共的数目：678648
FMM      分词正确的数目：270026
准确率（P）：39.78882 %
回归率（R）：47.20206 %
F 值为：0.43179564919821667
BMM 分词结果：
jieba    分词总共的数目：572064
BMM      分词总共的数目：678620
BMM      分词正确的数目：270798
准确率（P）：39.90422 %
回归率（R）：47.33701 %
F 值为：0.43303984059922407
</code></pre>
<p>得到新的词典拥有 74550 条词。接着，使用学习“新词”后的词典，对其余的资料进行分词，并评估分词结果。</p>
<p>代码：<a href="./test.py">test.py</a></p>
<figure>
<p><img src="./assets/python-set-after-train.png" alt="使用学习“新词”后的词典，FMM、BMM 法分词的结果" /></p>
<figcaption>图 4：使用学习“新词”后的词典，FMM、BMM 法分词的结果</figcaption>
</figure>
<pre><code>FMM 分词结果：
jieba    分词总共的数目：349662
FMM      分词总共的数目：368303
FMM      分词正确的数目：233505
准确率（P）：63.40024 %
回归率（R）：66.78020 %
F 值为：0.6504634627036137
BMM 分词结果：
jieba    分词总共的数目：349662
BMM      分词总共的数目：368354
BMM      分词正确的数目：233925
准确率（P）：63.50549 %
回归率（R）：66.90032 %
F 值为：0.6515871512612532
</code></pre>
<p>可以看到，虽然词典扩增规模不明显，但耗时显著增加。</p>
<h3 id="词典容量的影响"><a class="header" href="#词典容量的影响">词典容量的影响</a></h3>
<p>很显然，对于词典匹配法，当词典的容量越大，效果越好，进行分词所需的时间越多。</p>
<p>经过多次测试发现，在使用初始词典进行分词的情况下，正确率在 60% 左右。随着词典的中单词数量扩充至 17 ~ 18 万左右时，正确率可以达到 84% ~ 86%。</p>
<p>随着词典数量的进一步提升，分词的正确率的提升较小，但时间方面的开销却越来越大。</p>
<h2 id="六总结"><a class="header" href="#六总结">六、总结</a></h2>
<p>在这次实验中，练习了使用 FMM、BMM 以及 jieba 分词库对文本进行分词处理，在整个的实验过程中，通过查询相关资料等途径解决许多问题，实践能力得到了较大的提升。</p>
<p>仍存在正确分词个数统计不准确的问题，有待完善。</p>
<h3 id="关于如何计算正确匹配数"><a class="header" href="#关于如何计算正确匹配数">关于如何计算正确匹配数</a></h3>
<p>简单的方法是，</p>
<pre><code class="language-python">hits += len([i for i in result if i in truth]) 
miss += [i for i in result if i not in truth]
</code></pre>
<p>使用这种方法，当一句话中出现多个相同词汇时，可能会对结果造成影响。可以根据逗号（，）等符号将语句切分为不同的小段，并对每一个小段进行处理，可以较大程度上减少相同词汇出现的概率，一定程度上保证统计的正确率。</p>
<p>本次实验采用了如 <a href="./calc.py">calc.py</a> 中 <code>calc_hits</code> 方法的实现。</p>
<h2 id="参考资料"><a class="header" href="#参考资料">参考资料</a></h2>
<ul>
<li><a href="https://blog.csdn.net/u013510838/article/details/81673016">自然语言处理 1：分词 - CSDN</a></li>
<li><a href="https://blog.csdn.net/u013510838/article/details/81738431">自然语言处理 2：jieba 分词用法及原理 - CSDN</a></li>
<li><a href="https://github.com/fxsjy/jieba">fxsjy/jieba - GitHub</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/100552669">NLP 中文分词的评估指标 - 知乎</a></li>
<li><a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural language processing - Wikipedia</a></li>
<li><a href="https://stackoverflow.com/questions/11015320/how-to-create-a-trie-in-python/11015381">https://stackoverflow.com/questions/11015320/how-to-create-a-trie-in-python/11015381</a></li>
<li><a href="https://blog.csdn.net/weixin_41510260/article/details/104181827">《自然语言处理入门》笔记 - 2. 词典分词 - CSDN</a></li>
<li><a href="https://github.com/NLP-LOVE/Introduction-NLP">NLP-LOVE/Introduction-NLP: HanLP 作者的新书《自然语言处理入门》详细笔记</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../lab/01/index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="../../lab/03/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../lab/01/index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="../../lab/03/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
    </body>
</html>
