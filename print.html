<!DOCTYPE HTML>
<html lang="zh-Hans" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Learn NLP</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/css/custom-style.css">
        <link rel="stylesheet" href="theme/css/custom-font.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">首页</a></li><li class="chapter-item expanded "><a href="lab/index.html"><strong aria-hidden="true">1.</strong> 实验</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lab/01/index.html"><strong aria-hidden="true">1.1.</strong> 实验 1：互联网语料获取</a></li><li class="chapter-item expanded "><a href="lab/02/index.html"><strong aria-hidden="true">1.2.</strong> 实验 2：使用 FMM、BMM 算法分词</a></li><li class="chapter-item expanded "><a href="lab/misc/report-template.html"><strong aria-hidden="true">1.3.</strong> 实验报告模板</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Learn NLP</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/tsagaanbar/learn-NLP" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="自然语言处理学习"><a class="header" href="#自然语言处理学习">自然语言处理学习</a></h1>
<ul>
<li><a href="./lab/">自然语言处理实验</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="自然语言处理实验"><a class="header" href="#自然语言处理实验">自然语言处理实验</a></h1>
<ul>
<li>
<p><a href="lab/./misc/report-template.html">实验报告模板</a></p>
</li>
<li>
<p><a href="lab/./01/">实验 1：互联网语料获取</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="自然语言处理实验一"><a class="header" href="#自然语言处理实验一">自然语言处理：实验一</a></h1>
<p>编写程序自动抓取多网页内容</p>
<table>
    <style>
        th { text-justify: distribute; text-align-last: justify; text-align: justify; text-align: center; }
        td { text-align: center; }
    </style>
    <tbody>
        <tr><th>学院</th><td>信息工程学院</td></tr>
        <tr><th>指导教师</th><td>孙媛</td></tr>
        <tr><th>班级</th><td>19 级计算机科学与技术 1 班</td></tr>
        <tr><th>学生姓名</th><td>John Doe</td></tr>
        <tr><th>学号</th><td>19000000</td></tr>
    </tbody>
</table>
<p>日期： 2021 年 9 月 29 日</p>
<h2 id="摘要"><a class="header" href="#摘要">摘要</a></h2>
<p>略</p>
<h2 id="目录"><a class="header" href="#目录">目录</a></h2>
<p>略</p>
<h2 id="一实验内容"><a class="header" href="#一实验内容">一、实验内容</a></h2>
<p>学会使用 Python（或其他编程语言）编写网络爬虫程序，能对网站格式进行解析，爬取相关内容并过滤，建立语料库。</p>
<p>要求：能够自动爬取多个网页，过滤掉图片、视频信息，存取文中中的 <code>title</code>，<code>context</code>，<code>URL</code> 等关键文本字段，存取到不同的文本和目录中。</p>
<p>提交内容：</p>
<ul>
<li>实验报告</li>
<li>程序代码</li>
<li>爬取的语料</li>
</ul>
<p>本次实验选取了问答平台 <a href="https://zhihu.com">知乎</a> 作为实验对象。作为一个问答平台，知乎网站上内容的基本组成为问题，以及若干相应的答案；每个问题及问题下的回答均具有唯一的数字 URL 与之对应；此外，<a href="https://zhihu.com">知乎</a> 存在话题的概念，每个话题下可以包含一系列问题。</p>
<h2 id="二实验原理"><a class="header" href="#二实验原理">二、实验原理</a></h2>
<p>互联网上的网页通过超链接建立起彼此之间的联系。对于一个页面，通过检索其上存在的链接，并逐个访问这些链接、重复上述操作，就能够遍历该页面所在的整个网络。</p>
<p>传统的互联网页面的大多数元素通常包含在 HTML 文档源代码中，在了解了目标网站的 HTML 页面结构后，就能够如同在树上摘果子一般，从中提取自己需要的内容。</p>
<p>不过，现代网站多大量采用动态网页技术，以便更及时、更具针对性的向用户呈现内容，抑或作为一种反制网页爬取的对策。即，程序获取到的 HTML 源代码结构，并不是用户在浏览器中实际所看到的样子。一般来说，社交、购物网站经常使用这样的技术。若需从动态网站上获得信息，最理想的办法是使用网站方提供的应用程序接口；当这样的接口不存在时，则只能通过应用程序模拟真实用户使用浏览器时的操作，从而获得到页面上的元素。</p>
<p>除此之外，许多网站也会要求事先用户进行登录，之后才会提供更多信息。网站一般通过 Cookie 技术识别用户，因此可以通过在程序中使用正常登录后得到的 Cookie 来进行模拟登录。</p>
<h2 id="三整体框架"><a class="header" href="#三整体框架">三、整体框架</a></h2>
<p>实验目的是获取语料。在实验开始前，要先决定目标网站上感兴趣的信息，以及存储时使用的结构。对于本次实验的对象——知乎问答社区，感兴趣的信息主要是其上的问题以及相应的答案。不过为了使获取得到的内容更具相关性，选择一个话题作为研究对象更为经济。</p>
<p>根据前文所述，知乎问答社区的问题具有唯一的 URL，每个问题（或问题下的某个回答）可以视作信息抓取过程中最核心的着手对象。</p>
<p>截止实验时，进入知乎社区的话题页，即可以得到关于话题的简介，以及话题下的热门问题。</p>
<p>对于某一个话题下信息的存储，设计了如下的目录结构：</p>
<pre><code>TOPIC_ID
│
├── URL
├── DETAILS
├── SUMMARY
│
├── QUESTIONS
│   │
│   ├── QUESTION_ID_1
│   │   ├── ANSWER_ID_1
│   │   │   └── ANSWER_DETAILS
│   │   ├── ANSWER_ID_2
│   │   │   └── ANSWER_DETAILS
│   │   ....
│   │
│   ├── QUESTION_ID_2
│   │   ├── ANSWER_ID_1
│   │   │   └── ANSWER_DETAILS
│   │   ├── ANSWER_ID_2
│   │   │   └── ANSWER_DETAILS
│   │   ....
│
....
</code></pre>
<h2 id="四主要程序模块"><a class="header" href="#四主要程序模块">四、主要程序模块</a></h2>
<h3 id="程序依赖"><a class="header" href="#程序依赖">程序依赖</a></h3>
<p>引入依赖：</p>
<pre><code class="language-python">import json
import os
import time
import requests
from bs4 import BeautifulSoup
from pip._vendor.distlib.compat import raw_input
from selenium import webdriver
from urllib.error import HTTPError
</code></pre>
<ul>
<li>调用 Python 的 OS 模块用于操作主机的文件系统，用于目录建立、删除文件等操作。</li>
<li>使用 Time 模块进行计时，进而实现内容抓取操作之间具有一定的时间间隔，减少被反抓取对策侦测到的可能。</li>
<li>Requests 模块用于向指定 URL 发出请求，获得到页面源代码。</li>
<li>使用 BeautifulSoup 解析 HTML 文档的结构，以便对页面中指定内容进行提取。</li>
<li>Selenium 模块的 webdriver 类可以模拟用户使用浏览器时的操作，如拖动页面、点击等事件。</li>
</ul>
<h3 id="主要过程"><a class="header" href="#主要过程">主要过程</a></h3>
<p>程序主要过程如下：</p>
<pre><code class="language-python"># 备选话题链接：

urls = [
    &quot;https://www.zhihu.com/topic/19857419/hot&quot;, # 民大链接
    &quot;https://www.zhihu.com/topic/19606319/hot&quot;, # 川大链接
    &quot;https://wWw.zhihu.com/topic/19792394/hot&quot;, # 西南交大链接
    &quot;https://www.zhihu.com/topic/19687047/hot&quot;, # 西南财大链接
    &quot;https://www.zhihu.com/topic/19690586/hot&quot;, # 西南石油大学链接
    &quot;https://www.zhihu.com/topic/19694756/hot&quot;, # 川师链接
    &quot;https://www.zhihu.com/topic/19686215/hot&quot;  # 成都理工大链接
]

url = &quot;https://www.zhihu.com/topic/19857419/hot&quot;

header = {
    &quot;User-Agent&quot;: 
        &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) &quot;
        &quot;AppleWebKit/537.36 (KHTML, like Gecko) &quot;
        &quot;Chrome/80.0.3987.87 Safari/537.36&quot; }

soup = fetchPageAndMakeSoup(url, header)

hrefs, paths = getFirstTitle(soup)

# 取得热门回答的链接及写入路径
href = hrefs[3]
path = paths[3]

dealWithTopAnswers(href, path)
</code></pre>
<h3 id="子过程介绍"><a class="header" href="#子过程介绍">子过程介绍</a></h3>
<p><code>fetchPageAndMakeSoup</code> 能够以指定的 <code>header</code> 爬取指定 HTTP 协议下某 URL 对应的页面源代码。之后，根据页面内容创建一个 <code>BeautifulSoup</code> 对象供后续分析使用。</p>
<pre><code class="language-python">from urllib.error import HTTPError

def fetchPageAndMakeSoup(url, header):
    try:
        r = requests.get(url, headers=header)
        r.encoding = r.apparent_encoding
        content = r.text
    except HTTPError as e:
        return None
    bsObject = BeautifulSoup(content, &quot;lxml&quot;)
    return bsObject
</code></pre>
<hr />
<figure>
    <img src="lab/01/assets/MUC-topic-page.png" alt="中央民族大学话题主页">
    <figcaption>图 1：中央民族大学话题主页</figcaption>
</figure>
<p>知乎话题页下的若干板块如图所示：</p>
<figure class="image">
    <img src="lab/01/assets/topic-page-tabs.png" alt="知乎话题页下的若干板块">
    <figcaption>图 2：知乎话题页下的若干板块</figcaption>
</figure>
<p><code>getFirstTitle</code> 可以从知乎话题页分析出主要的几个板块，及其对应的链接。</p>
<pre><code class="language-python">def getFirstTitle(soup):
    # 获取一级标题：索引、简介、讨论

    titles_href = []
    titles_path = [] # 实际上存的是一级标题的路径噢！

    data = soup.find(&quot;div&quot;, id=&quot;TopicMain&quot;)
    title = data.find_all(&quot;li&quot;, class_=&quot;Tabs-item Tabs-item--noMeta&quot;)
    
    for each in title:
        print(each.find(&quot;a&quot;).contents)      # 获取 TITLE 内容：索引、简介、讨论、精华、等待回答
        print(each.find(&quot;a&quot;)['href'])       # 需要加上 zhihu.com前缀

        path_of_folder = &quot;.&quot; + &quot;/&quot; + &quot;&quot;.join(each.find(&quot;a&quot;).contents)
        createFolder(path_of_folder)

        href = &quot;http://zhihu.com&quot; + each.find(&quot;a&quot;)['href']
        writeHrefToFiles(path_of_folder + &quot;/&quot; + &quot;href.txt&quot;, href)

        titles_href.append(href)
        titles_path.append(path_of_folder)
    
    return titles_href, titles_path
</code></pre>
<p>该函数在当前路径下（<code>.py</code> 脚本的运行处）创建目录，名称于上述二级标题相同，并且将各个 URL 以文本形式存放在相应的文件夹中。同时，通过 <code>titles_href</code> 和 <code>titles_path</code> 来对“URL 链接”和“存储路径”进行存储，以便进行后续的操作。</p>
<hr />
<p><code>createFolder</code> 函数首先判断路径下是否存在该同名目录，若不存在，则调用 <code>os</code> 模块的 <code>makedirs()</code> 方法进行创建。</p>
<pre><code class="language-python">def createFolder(path):
    # 创建文件夹
    folder = os.path.exists(path)
    if not folder:
        os.makedirs(path)  # makedirs 创建文件时如果路径不存在会创建这个路径
        print(&quot;---  new folder...  ---&quot;)
        print(&quot;---  OK  ---&quot;)

</code></pre>
<hr />
<p><code>writeHrefToFiles</code> 将 href 链接写入指定路径的 TXT 文件中。</p>
<pre><code class="language-python">def writeHrefToFiles(path, href):
    # 将 href 链接写入指定路径的 TXT 文件中
    # 使用 with open ... as f 可以保证资源在使用完成后得到释放
    with open(path, &quot;w&quot;) as f:
        f.write(href)
</code></pre>
<hr />
<p>处理“<strong>索引</strong>”标签下的内容，主要目的为获取二级标题，问题标题。获取二级标题后，便于对各个问题进行分类，便于后续进一步的处理。</p>
<figure>
    <img src="lab/01/assets/topic-page-index-tab.png" alt="索引选项卡下的二级标题">
    <figcaption>图 3：索引选项卡下的二级标题</figcaption>
</figure>
<pre><code class="language-python"># 获取二级标题，问题标题
# 获取二级标题后，便于对各个问题进行分类，便于后续进一步的处理。
def getSecondTitle(soup, path):
    
    # 首先通过 class 标签获取二级标题，如下：
    data = soup.find(&quot;div&quot;, class_=&quot;Card TopicIndex-content&quot;)
    # details = data.find_all(&quot;h2&quot;, class_ = &quot;TopicIndexModule-title&quot;)
    details = data.find_all(&quot;div&quot;, class_=&quot;TopicIndexModule&quot;)

    index = 1
    driver = webdriver.Firefox()

    # 通过相应的 class 获取到所有文章内容后，通过区间 for 循环遍历所有内容
    for val in details:
        # 索引标题下的子标题：就读体验，校园设施…………
        # 具体的标签如同 div h2 或者是 class 名 随着页面变化而变化
        # 此处是对大学页面下的索引进行处理。
        second_title = val.find(&quot;h2&quot;, class_=&quot;TopicIndexModule-title&quot;)
        print(second_title.contents)
        articles_titles = val.find_all(&quot;div&quot;, class_=&quot;TopicIndexModule-item&quot;)  # 多个标题
        path_of_second_title = path + &quot;/&quot; +&quot;&quot;.join(second_title.contents)
        createFolder(path_of_second_title)

        for article in articles_titles:
            article_name = article.find(&quot;a&quot;).contents  # 每个标题  列表形式
            article_href = article.find(&quot;a&quot;)['href']  # 附加的 href
            print(article_name)
            print(article_href)
            print(path_of_second_title)
            path_of_article_name = path_of_second_title + &quot;/&quot; + &quot;&quot;.join(article_name)
            path_of_article_name = dealWithPath(path_of_article_name)
            print(path_of_article_name)
            createFolder(path_of_article_name)
            href = &quot;http://zhihu.com&quot; + article_href

            # 获取了回答的名称以及链接以后
            # 由于路径可能存在不被文件系统允许的特殊符号
            # 因此需要对文件名称进行一定的处理
            path_of_href = dealWithPath(path_of_article_name + &quot;/&quot; + &quot;href.txt&quot;)
            writeHrefToFiles(path_of_href, href)

            # 在存储了具体的以问题名称为名的文件夹和其 URL后，
            # 我们就可以对问题页面进行处理并且提取其中各个用户的回答，代码如下：
            time.sleep(3)
            driver.get(href)

            # 由于知乎的问题页面存在动态加载，
            # 因此需要通过 selenium 来模拟浏览器进行向下滑动页面的操作
            # 以获取所有的回答，通过 simulatePageScrolling()函数进行实现。
            simulatePageScrolling(driver ,20)
            html = driver.page_source
            soup1 = BeautifulSoup(html, 'lxml')

            # 将内容写入 TXT 文件
            writeContentToFiles(soup1, path_of_article_name)
</code></pre>
<p><code>dealWithPath</code> 函数：<code>special_characters</code> 这一列表中包含文件系统路径不可使用的特殊符号，则对路径进行遍历，删除其中存在的特殊符号即可。</p>
<pre><code class="language-python">def dealWithPath(path):
    # 删除路径中可能不合法的特殊字符
    special_characters = [&quot;?&quot;, &quot;/&quot;, &quot;:&quot;, &quot;*&quot;, &quot;&lt;&quot;, &quot;&gt;&quot;, &quot;|&quot;]
    for each in special_characters:
        path = path.replace(each, &quot;&quot;)
    return path
</code></pre>
<p><code>simulatePageScrolling</code> 模拟浏览器进行向下滑动页面的操作。</p>
<pre><code class="language-python">def simulatePageScrolling(driver, times):
    for i in range(times + 1):
        driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;)
        time.sleep(1)
</code></pre>
<hr />
<p>将回答写入TXT的具体实现过程如下：</p>
<pre><code class="language-python">def writeContentToFiles(soup, path):
    articles = soup.find_all(&quot;div&quot;, class_=&quot;RichContent-inner&quot;)
    index = 1
    tag = 0
    for val in articles:
        to_path = path + &quot;/&quot; + &quot;回答&quot; + str(index) + &quot;.txt&quot;
        index += 1
        # if val.find(&quot;p&quot;).string is None:
        #     continue   # 无内容， 不进行处理
        with open(to_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            for para in val.find_all(&quot;p&quot;):
                if para.string is not None:
                    f.write(para.string)
                    f.write(&quot;\n&quot;)

        if os.stat(to_path).st_size == 0:
            os.remove(to_path)   # 删除空文件
</code></pre>
<p>此处需要注意打开文件与 <code>for</code> 循环的位置，否则会出现将多个问题写入同一个 TXT 文件中、或着将一个问题拆分成多个 TXT 文件进行存储的情况。</p>
<p>通过 <code>index</code> 来区分不同的回答，同时避免在写入文件时发生重名的现象。通过 <code>os</code> 模块来对文件的大小进行判断，若其大小为 0（空数据），则将该文件删除，同时应注意，此处的删除文件操作应在 <code>with open</code> 对该文件的操作完全结束后再进行。</p>
<hr />
<p>接下来对简介二级标题进行操作：</p>
<figure>
    <img src="lab/01/assets/topic-page-intro-tab.png" alt="话题页下的简介选项卡">
    <figcaption>图 4：话题页下的简介选项卡</figcaption>
</figure>
<pre><code class="language-python">def dealWithIntro(href, path):
    path_of_href = path + &quot;\\&quot; + &quot;href.txt&quot;
    path_of_txt = path + &quot;\\&quot; + &quot;简介.txt&quot;

    with open(path_of_href, &quot;w&quot;, encoding = &quot;utf-8&quot;) as f:
        f.write(href)

    url_intro = href
    r = requests.get(url_intro, headers=header)
    r.encoding = r.apparent_encoding
    content = r.text
    soup = BeautifulSoup(content, &quot;lxml&quot;)
    #print(soup)
    intro_names = soup.find_all(&quot;div&quot;, class_=&quot;TopicCommonIntroTable-item&quot;)

    with open(path_of_txt, &quot;w&quot;, encoding = &quot;utf-8&quot;) as f:
        for val in intro_names:
            intro_name = val.find(&quot;div&quot;, class_=&quot;TopicCommonIntroTable-name&quot;)
            intro_attributes = val.find(&quot;span&quot;, class_=&quot;TopicCommonField-topicText&quot;)
            if intro_attributes is None:
                intro_attributes = val.find(&quot;div&quot;, class_=&quot;TopicCommonField-text&quot;)
            print(intro_name.contents)
            print(intro_attributes.contents)
            intro_name_str = &quot;&quot;.join(intro_name.contents).rstrip(&quot;\n&quot;)
            #print(intro_name_str)
            f.write(intro_name_str)
            #f.write(&quot;&quot;.join(intro_name.contents).rstrip(&quot;\n&quot;))
            f.write(&quot;:&quot;+&quot; &quot;)
            #intro_attributes_str = &quot;&quot;.join(intro_attributes.contents)
            if intro_name_str == &quot;学校官网&quot;:                                        #进行特判
                intro_attributes_str = intro_attributes.find(&quot;a&quot;)['href']
            else:
                intro_attributes_str = &quot;&quot;.join(intro_attributes.contents)
            #print(intro_attributes_str)
            f.write(intro_attributes_str)
            #f.write(&quot;&quot;.join(intro_attributes.contents))
            f.write(&quot;\n&quot;)
</code></pre>
<p>大部分操作于上文相同，但此处存在相同位置的内容标签不同的情况，需要对其进行特判。同时，由于学校官网的编码形式可能与其他内容的编码不同，所以在打开文件进行写操作时需要统一编码格式为“UTF-8”。</p>
<hr />
<p>对“精华”选项卡进行处理：</p>
<figure>
    <img src="lab/01/assets/topic-page-selected-tab.png" alt="话题页下的精华选项卡">
    <figcaption>图 5：话题页下的精华选项卡</figcaption>
</figure>
<pre><code class="language-python">def dealWithTopAnswers(href, path):     #处理精华标题
    driver = webdriver.Firefox()
    driver.get(href)
    simulatePageScrolling(driver, 20)
    
    html = driver.page_source
    soup = BeautifulSoup(html, 'lxml')

    # 得到热门问题的列表
    articles = soup.find_all(&quot;div&quot;, itemtype=&quot;http://schema.org/Question&quot;)

    print(articles)

    for val in articles:
        question_href = val.find(&quot;a&quot;)['href']
        quesiton_title = val.find(&quot;a&quot;).contents
        question_id = question_href[question_href.find('question/')+len('question/'):]

        print(question_href)
        print(quesiton_title)

        path_of_question = dealWithPath(path + &quot;/&quot; + &quot;&quot;.join(quesiton_title))
        print(path_of_question)

        createFolder(path_of_question)

        href = &quot;https:&quot; + question_href
        path_of_href = dealWithPath(path_of_question + &quot;/&quot; + &quot;href.txt&quot;)

        # href be like: https://www.zhihu.com/question/28580571

        writeHrefToFiles(path_of_href, href)

        time.sleep(3)

        driver.get(href)
        simulatePageScrolling(driver, 20)

        # 得到某一热门问题的页面内容
        html = driver.page_source
        soup1 = BeautifulSoup(html, 'lxml')

        # 将内容存储到文件
        # path_of_answers = path_of_article_name + &quot;\\&quot; + &quot;回答&quot; + str(index) + &quot;.txt&quot;
        writeContentToFiles(soup1, path_of_question)
</code></pre>
<p>大部分操作与对索引进行的操作相同，变化的仅是标签的属性，则不再进一步的阐述。</p>
<hr />
<p>实验结果如下：</p>
<figure>
    <img src="lab/01/assets/result-file-tree-2.png" alt="爬取结果目录结构">
    <figcaption>图 6：爬取结果目录结构 1</figcaption>
</figure>
<figure>
    <img src="lab/01/assets/result-file-tree-3.png" alt="爬取结果目录结构">
    <figcaption>图 7：爬取结果目录结构 2</figcaption>
</figure>
<figure>
    <img src="lab/01/assets/result-file-content.png" alt="爬取结果内容展示">
    <figcaption>图 8：爬取结果内容展示</figcaption>
</figure>
<p>另注：由于“讨论”和“等待回答”选项卡中会出现大量的同之前相同的提问，因此不再对这两个二级标题进行爬取，一方面可以加快程序的运行事件，同时减少数据的冗余量和重复度。</p>
<h2 id="五实验结果"><a class="header" href="#五实验结果">五、实验结果</a></h2>
<p>详细分析实验结果，除了包含定量评价，还要有定性评价。
对存在的问题，要着重剖析。</p>
<h3 id="实验结果的展示"><a class="header" href="#实验结果的展示">实验结果的展示</a></h3>
<p>本次爬虫选取的是主题是中国部分大学，选取的在知乎上较为有热度的回答。</p>
<p>以下使用的图片皆使用中央民族大学进行举例，其他高校结构以及爬去结果相同。</p>
<p>在每个文件夹依据大学主题的 5 个分支建立 5 个分文件夹，分别是索引、简介、讨论、精华和等待回答。</p>
<figure>
    <img src="lab/01/assets/MUC-topic-page.png" alt="中央民族大学话题主页">
    <figcaption>图 9：中央民族大学话题主页</figcaption>
</figure>
<p>注：由于等待回答、讨论并非重点内容，所以文件夹下面仅有存放相应链接的 href.txt 文件。</p>
<h4 id="简介"><a class="header" href="#简介">简介</a></h4>
<p>简介下存储相应链接的 <code>href.txt</code> 文件和存放简介的 <code>简介.txt</code>。</p>
<h4 id="精华"><a class="header" href="#精华">精华</a></h4>
<p>在精华下是由问题名称构成的文件夹名。</p>
<h4 id="索引"><a class="header" href="#索引">索引</a></h4>
<p>在索引下是由其他专业、就读体验、校园生活、校园设施、毕业发展、热门专业和考研相关这几个子话题文件夹构成，每个子话题文件夹下面都有很多问题，其文件夹名是由问题名称构成的，并配有存储相应链接的 <code>href.txt</code> 文件。</p>
<h4 id="问题"><a class="header" href="#问题">问题</a></h4>
<p>每个问题下是由 txt 文本构成的回答：<code>回答1.txt</code> <code>回答2.txt</code> <code>回答3.txt</code>，以及存储相应链接的 <code>href.txt</code> 文件</p>
<h3 id="实验结果的分析"><a class="header" href="#实验结果的分析">实验结果的分析</a></h3>
<p>本次实验爬取部分高校在知乎问答平台上收到的一部分评价作为语料，在练习网页抓取技术的同时，也为未来自然语言处理时提供了非常重要的原始材料。</p>
<h3 id="存在的问题"><a class="header" href="#存在的问题">存在的问题</a></h3>
<p>爬取的问题比较少，信息量还有待提高，如点赞数、评论数等，未来进行舆情分析时可以增添更多可分析的要素。</p>
<h2 id="六总结"><a class="header" href="#六总结">六、总结</a></h2>
<p>本次实验选择知乎问答社区为内容抓取对象。首先，程序读入待抓取的话题页链接或话题页链接列表。随后，对于每一个链接，采用 UA 模拟的方式获取到页面源代码。话题页中可以得到关于话题的简介、以及热门问题的链接。获取到以上二者后，再次抓取热门问题页面的内容，并写入到本地文件。</p>
<h2 id="参考文献"><a class="header" href="#参考文献">参考文献</a></h2>
<ul>
<li><a href="https://ithelp.ithome.com.tw/articles/10225429">https://ithelp.ithome.com.tw/articles/10225429</a></li>
<li><a href="https://marcovaldong.github.io/2016/08/18/Python%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E7%9F%A5%E4%B9%8E%E5%B0%8F%E7%BB%93/">https://marcovaldong.github.io/2016/08/18/Python爬虫爬取知乎小结/</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="自然语言处理"><a class="header" href="#自然语言处理">自然语言处理</a></h1>
<p style="text-align: center;">实验二</p>
<img src="lab/02/./assets/logo.svg" alt="School Logo" style="max-width: 30%;">
<!-- {{#include ../misc/authors.html}} -->
<table>
    <style>
        th { text-justify: distribute; text-align-last: justify; text-align: justify; text-align: center; }
        td { text-align: center; }
    </style>
    <tbody>
        <tr><th>学院</th><td>信息工程学院</td></tr>
        <tr><th>指导教师</th><td>孙媛</td></tr>
        <tr><th>班级</th><td>19 级计算机科学与技术 1 班</td></tr>
        <tr><th>学生姓名</th><td>John Doe</td></tr>
        <tr><th>学号</th><td>19000000</td></tr>
    </tbody>
</table>
<p style="text-align: center;">日期： 2021 年 10 月 21 日</p>
<div style="page-break-after: always;"></div>
<h2 id="目录-1"><a class="header" href="#目录-1">目录</a></h2>
<ol style="list-style-type: none;">
    <li><a href="lab/02/index.html#自然语言处理实验二">自然语言处理：实验二</a>
        <ol style="list-style-type: none;">
            <li><a href="lab/02/index.html#一实验内容">一、实验内容</a></li>
            <li><a href="lab/02/index.html#二实验原理">二、实验原理</a>
                <ol style="list-style-type: none;">
                    <li><a href="lab/02/index.html#基于词典的切分方法">基于词典的切分方法</a></li>
                    <li><a href="lab/02/index.html#jieba-分词库">jieba 分词库</a></li>
                    <li><a href="lab/02/index.html#扩充词典">扩充词典</a></li>
                    <li><a href="lab/02/index.html#准确率精确率召回率及-F-值">准确率、精确率、召回率及 F 值</a></li>
                    <li><a href="lab/02/index.html#分词结果的评估">分词结果的评估</a></li>
                </ol>
            </li>
            <li><a href="lab/02/index.html#三整体框架">三、整体框架</a></li>
            <li><a href="lab/02/index.html#四主要程序模块">四、主要程序模块</a>
                <ol style="list-style-type: none;">
                    <li><a href="lab/02/index.html#fmmbmm">FMM、BMM</a></li>
                    <li><a href="lab/02/index.html#评估分词结果">评估分词结果</a></li>
                    <li><a href="lab/02/index.html#多文件处理">多文件处理</a></li>
                    <li><a href="lab/02/index.html#交互式分词程序">交互式分词程序</a></li>
                    <li><a href="lab/02/index.html#训练及测试">训练及测试</a></li>
                    <li><a href="lab/02/index.html#trie-模块">Trie 模块</a></li>
                </ol>
            </li>
            <li><a href="lab/02/index.html#五实验结果">五、实验结果</a>
                <ol style="list-style-type: none;">
                    <li><a href="lab/02/index.html#基本实验结果">基本实验结果</a></li>
                    <li><a href="lab/02/index.html#词典容量的影响">词典容量的影响</a></li>
                </ol>
            </li>
            <li><a href="lab/02/index.html#六总结">六、总结</a>
                <ol style="list-style-type: none;">
                    <li><a href="lab/02/index.html#关于如何计算正确匹配数">关于如何计算正确匹配数</a></li>
                    <li><a href="lab/02/index.html#参考资料">参考资料</a></li>
                </ol>
            </li>
        </ol>
    </li>
</ol>
<div style="page-break-after: always;"></div>
<h2 id="一实验内容-1"><a class="header" href="#一实验内容-1">一、实验内容</a></h2>
<p>实验内容：</p>
<ul>
<li>对语料库的文本进行分词并存储。</li>
<li>分别采用正向最大匹配算法、逆向最大匹配算法进行分词。以 <a href="https://github.com/fxsjy/jieba">jieba</a> 分词的分词结果作为标准语料，计算P、R、F值。</li>
</ul>
<h2 id="二实验原理-1"><a class="header" href="#二实验原理-1">二、实验原理</a></h2>
<h3 id="基于词典的切分方法"><a class="header" href="#基于词典的切分方法">基于词典的切分方法</a></h3>
<p>句子 \(S = c_1 c_2 \cdots c_n\)：句子由若干字符 \(c\) 组成。</p>
<p>假设词 \(w_i = c_1 c_2 \cdots c_m\)，其中 \(m\) 为词典中最长词的字数。</p>
<p>当前的分词算法主要分为两类——基于词典的规则匹配方法和基于统计的机器学习方法。</p>
<p>基于词典的分词算法，本质上就是字符串匹配。将待匹配的字符串基于一定的算法策略，和一个足够大的词典进行字符串匹配，如果匹配命中，则可以分词。根据不同的匹配策略，又分为正向最大匹配法，逆向最大匹配法，双向匹配分词，全切分路径选择等。</p>
<p>最大匹配法（Maximum Matching, MM）主要分为三种：</p>
<ul>
<li>正向最大匹配算法（Forward MM, FMM）：从左到右对语句进行匹配，匹配的词越长越好。这种方式切分会有歧义问题出现。</li>
<li>逆向最大匹配算法（Backward MM, BMM）：从右到左对语句进行匹配，同样也是匹配的词越长越好。这种方式同样也会有歧义问题。</li>
<li>双向最大匹配算法（Bi-directional MM）：则同时采用正向最大匹配和逆向最大匹配，选择二者分词结果中词数较少者。但这种方式同样会产生歧义问题。由此可见，词数少也不一定划分就正确。</li>
</ul>
<p><strong>全切分路径选择</strong>，将所有可能的切分结果全部列出来，从中选择最佳的切分路径。分为两种选择方法：</p>
<ol>
<li>n 最短路径方法。将所有的切分结果组成有向无环图，切词结果作为节点，词和词之间的边赋予权重，找到权重和最小的路径即为最终结果。比如可以通过词频作为权重，找到一条总词频最大的路径即可认为是最佳路径。</li>
<li>n 元语法模型。同样采用 n 最短路径，只不过路径构成时会考虑词的上下文关系。一元表示考虑词的前后一个词，二元则表示考虑词的前后两个词。然后根据语料库的统计结果，找到概率最大的路径。</li>
</ol>
<p>此次实验默认使用的是实验指导书中提供的词典，分别采用正向最大匹配算法、逆向最大匹配算法进行分词。</p>
<h3 id="jieba-分词库"><a class="header" href="#jieba-分词库">jieba 分词库</a></h3>
<p><a href="https://github.com/fxsjy/jieba">jieba</a> 支持三种分词模式:</p>
<ol>
<li>精确分词，试图将句子最精确的切开，适合文本分析</li>
<li>全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义</li>
<li>搜索引擎模式，在精确模式基础上，对长词进行再次切分，提高recall，适合于搜索引擎。</li>
</ol>
<h3 id="扩充词典"><a class="header" href="#扩充词典">扩充词典</a></h3>
<p>将过程中遇到的“新词”添加进词典，可以提高文本识别的准确率。</p>
<h3 id="准确率精确率召回率及-f-值"><a class="header" href="#准确率精确率召回率及-f-值">准确率、精确率、召回率及 F 值</a></h3>
<p>机器学习中的分类评估包含有以下这么几个概念。</p>
<p><strong>准确率</strong>（Accuracy），即正确分类的数量占总的数量的比值，是一个用来衡量分类器预测结果与真实结果差异的一个指标，越接近于 1 说明分类结果越准确。</p>
<p>二分类的结果有以下几种可能性：</p>
<ul>
<li>True Positive（TP）：表示将正样本预测为正样本，即预测正确；</li>
<li>False Positive（FP）：表示将负样本预测为正样本，即预测错误；</li>
<li>False Negative（FN）：表示将正样本预测为负样本，即预测错误；</li>
<li>True Negative（TN）：表示将负样本预测为负样本，即预测正确；</li>
</ul>
<p><strong>精确率</strong>（Precision）计算的是预测对的正样本在整个预测为正样本中的比重，而<strong>召回率</strong>（Recall）计算的是预测对的正样本在整个真实正样本中的比重。因此，一般来说，召回率越高，意味着模型找寻正样本的能力越强。</p>
<p>准确率、精确率、召回率的计算公式如下：</p>
<p>\[
\begin{align}
\text{Accuracy} = &amp; \frac{TP+TN}{TP+FP+FN+TN} \\
\text{Precision} = &amp; \frac{TP}{TP+FP} \\
\text{Recall} = &amp; \frac{TP}{TP+FN} \\
\end{align}
\]</p>
<p>值得注意的是，在实际任务中，并不明确哪一类是正样本或哪一类是负样本，所以对于每个类别，都可以计算其各项指标。</p>
<p>实际评估一个系统时，应同时考虑 P 和 R，但同时要比较两个数值，很难做到一目了然。所以常采用综合两个值进行评价的办法，综合指标 F 值就是其中一种。计算公式如下：</p>
<p>\[
\text{F-score} = (1+\beta^2)\frac{P \times R}{\beta^2 \times P + R}
\]</p>
<p>其中，\(\beta\) 决定对 P 侧重还是对 R 侧重，通常设定为 1、2 或 \(\frac 1 2\)。\(\beta\) 取值为 1，即对二者一样重视，这时的 F-score 称为 \(F_1\) 值。</p>
<h3 id="分词结果的评估"><a class="header" href="#分词结果的评估">分词结果的评估</a></h3>
<p>机器学习中二分类的评估标准，无法直接应用于分词。</p>
<p>在对汉语分词性能进行评估时，采用了常用的３个评测指标：准确率（P）、召回率（R）、综合指标 F 值（F）。准确率表示在切分的全部词语中，正确的所占的比值。召回率指在所有切分词语中（包括切分的和不应该忽略的），正确切分的词语所占的比值。准确率描述系统切分的词语中，正确的占多少。召回率表示应该得到的词语中，系统正确切分出了多少。计算公式如下：</p>
<p>\[
P = \frac{\text{准确切分的词语数}}{\text{切分出的所有词语数}}
\]</p>
<p>\[
R = \frac{\text{准确切分的词语数}}{\text{应该切分的词语数}}
\]</p>
<p>若一字符串的分词结果为一系列单词，设每个单词按照其在文中的起止位置可记作区间 \([i,j]\)（\(0\leq i \leq j \leq n\)），那么标准答案对应的所有区间就可以构成一集合 \(A\)，作为正类，其他的区间则作为负类;同理，根据分词结果，可以得到集合 \(B\)。</p>
<p>\[TP \cup FN = A \]</p>
<p>\[TP \cup FP = B \]</p>
<p>\[A \cap B = TP \]</p>
<p>则对于分词结果，P、R 的计算公式：</p>
<p>\[\text{Precision} = \frac{\vert A\cap B\vert}{\vert B \vert}\]</p>
<p>\[\text{Recall} = \frac{\vert A\cap B\vert}{\vert A \vert}\]</p>
<h2 id="三整体框架-1"><a class="header" href="#三整体框架-1">三、整体框架</a></h2>
<p>主要分为功能模块和使用调用。详见主要程序模块。</p>
<ul>
<li>FMM、BMM 分词：<a href="lab/02/./FMM_BMM.py">FMM_BMM.py</a>、<a href="lab/02/./FMM_BMM_trie.py">FMM_BMM_trie.py</a></li>
<li>多文件处理：<a href="lab/02/./file_processing.py">file_processing.py</a></li>
</ul>
<h2 id="四主要程序模块-1"><a class="header" href="#四主要程序模块-1">四、主要程序模块</a></h2>
<h3 id="fmmbmm"><a class="header" href="#fmmbmm">FMM、BMM</a></h3>
<p>从文件中加载字典到 Python 内建的 <code>set</code> 类型，以及进行最大正向匹配（FMM）、最大逆向匹配（BMM）的功能：</p>
<p><a href="lab/02/./FMM_BMM.py">FMM_BMM.py</a> </p>
<pre><code class="language-python">def load_wordlist(filename):
    wordlist = set()
    maxlen = 0
    cnt = 0
    with open(filename) as f:
        # 判断文件首行是否为指定的 'magic code'
        if f.readline().strip() == '@Lexicon':
            for line in f:
                word = line.split()[1]
                maxlen = max(maxlen, len(word))
                wordlist.add(word)
                cnt += 1
    # maxlen 为字典中最长词的长度
    return wordlist, cnt, maxlen


def add_words(wordlist, cnt, maxlen, new_words):
    for w in new_words:
        w = w.strip()
        if len(w) == 0:
            continue
        if w.startswith(&quot;-&quot;) or w.startswith(&quot;.&quot;):
            # 排除可能的特殊符号串，如“----------”
            continue
        if w.isdigit():
            # 同理，排除掉特殊情况，使得字典更具有普遍性
            continue
        cnt += 1
        maxlen = max(len(w), maxlen)
        wordlist.add(w)
    return wordlist, cnt, maxlen


def save_wordlist(filename, wordlist):
    with open(filename, &quot;w&quot;, encoding='utf-8') as out:
        out.write('@Lexicon\n')
        cnt = 0
        for w in wordlist:
            cnt += 1
            out.write(str(cnt) + ' ' + w + '\n')


def FMM(sentence, wordlist, maxlen):
    maxlen = max(1, maxlen)
    tokens = []
    i = 0
    while i &lt; len(sentence):
        n = len(sentence) - i # 未被切分的字串长度
        m = min(maxlen, n)
        w = sentence[i:i+m]
        while len(w) &gt; 1:
            if w in wordlist:
                break
            else:
                w = w[0:-1]
        tokens.append(w)
        i += len(w)
    return tokens


def BMM(sentence, wordlist, maxlen):
    maxlen = max(1, maxlen)
    tokens = []
    i = len(sentence)
    while i &gt;= 1:
        n = i # 未被切分的字串长度
        m = min(maxlen, n)
        w = sentence[i-m:i]
        while len(w) &gt; 1:
            if w in wordlist:
                break
            else:
                w = w[1:]
        tokens.append(w)
        i -= len(w)
    tokens.reverse()
    return tokens

</code></pre>
<h3 id="评估分词结果"><a class="header" href="#评估分词结果">评估分词结果</a></h3>
<p>评估分词结果，以及计算 P、R、F 值：</p>
<p><a href="lab/02/./calc.py">calc.py</a></p>
<pre><code class="language-python">special_characters = set(list(
    &quot;()[]+-*/&lt;&gt;|\\;:\&quot;\'\,.?!@#$%^&amp;~`\{\}（）【】《》，。？“”‘’；：——「」『』〔〕&quot;
))

#包括部分特殊字符，在进行分词比对时将特殊字符排除，以免对结果产生一定的影响

def calc_hits(truth, result):
    # 传入分词得到的结果（列表），以及“正确分词”结果
    cut_truth = [item for item in truth if item not in special_characters]
    cut_result = [item for item in result if item not in special_characters]
    i = 0       # 指向 truth 中的 token
    j = 0       # 指向 result 中的 token
    l1 = 0      # i 所指向词串，对应在原句中的长度
    l2 = 0      # j 所指向词串，对应在原句中的长度
    hits = 0
    missmatches = set()
    while i &lt; len(cut_truth) and j &lt; len(cut_result):
        if l1 &lt; l2:
            l1 += len(cut_truth[i])
            i += 1
        elif l1 &gt; l2:
            l2 += len(cut_result[j])
            j += 1
        else: 
            if cut_truth[i] == cut_result[j]:
                hits += 1
            else:
                # 记录未匹配到的“新词”
                missmatches.add(cut_truth[i])
            l1 += len(cut_truth[i])
            i += 1
            l2 += len(cut_result[j])
            j += 1
    
    return hits, len(truth), len(result), missmatches


def calc_PRF(hits, truth_len, result_len):
    P = hits / result_len       # precision
    R = hits / truth_len        # recall
    F = (2 * P * R) / (P + R)   # F_1
    return P, R, F
</code></pre>
<h3 id="多文件处理"><a class="header" href="#多文件处理">多文件处理</a></h3>
<p><a href="lab/02/./file_processing.py">file_processing.py</a></p>
<pre><code class="language-python"># -*-coding:UTF-8 -*-
import os
import time

def cut_txt(file_name, cut_methods_list):
    &quot;&quot;&quot;
    单个TXT文档处理，可以接收多个方法，
    :param file_name:   要处理的文件名
    :param cut_methods_list: 类似如下的列表 [(&quot;name&quot;, method), ...]
    :return: 存储分词结果的字典，以方法名作为键名
    &quot;&quot;&quot;
    results = dict()
    elapsed_time = 0
    with open(file_name, &quot;r&quot;, encoding='utf-8') as f:
        for line in f:
            for pair in cut_methods_list:
                foo_name = pair[0]
                foo = pair[1]
                start = time.time()
                results[foo_name] = foo(line.strip())
                end = time.time()
                elapsed_time += (end - start)

    return results, elapsed_time


def write_results(file_name, results, delimiter='/', output_dir_prefix=''):
    &quot;&quot;&quot;
    将分词结果写出到文件
    &quot;&quot;&quot;
    for foo_name in results.keys():
        out_file_name = os.path.join(
            output_dir_prefix, 
            file_name + &quot;.&quot; + foo_name + &quot;.segmented&quot;)
        with open(out_file_name, 'w', encoding='utf-8') as out:
            out.write(
                delimiter.join(results[foo_name]))


def process_path(path, cut_methods_list):
    &quot;&quot;&quot;
    对目录下的所有文件进行处理
    :param path: 目录名
    :param cut_methods_list: 要采用的分词方法的列表
    :return: 文件名及对应结果的生成器
    &quot;&quot;&quot;
    ignore = ['href', '简介', 'segmented', '#']
    for root, subdirs, files in os.walk(path):
        for f in files:
            file_name = os.path.join(root, f)

            # 如果文件名含有某些特征，跳过
            should_pass = False
            for kw in ignore:
                if file_name.find(kw) != -1:
                    should_pass = True
                    break
            if should_pass: continue

            # 当前正在处理的文件
            print(' - Processing:', file_name)
            
            # 获得各方法的分词结果：results
            
            results, t = cut_txt(file_name, cut_methods_list)

            yield file_name, results, t 

</code></pre>
<p>在 <code>简介.txt</code> 和 <code>href.txt</code> 中含有 URL 链接，对其进行处理会在一定程度上影响到分词的结果，亦会产生较多无意义的“新词”，故不对其进行处理。</p>
<h3 id="交互式分词程序"><a class="header" href="#交互式分词程序">交互式分词程序</a></h3>
<p><a href="lab/02/./demo.py">demo.py</a></p>
<pre><code class="language-python">from FMM_BMM_trie import *
from calc import *

import jieba

wordlist, cnt, maxlen = load_wordlist('wordlist.dic')

while True:
    s = input().strip()
    if s == '#':
        break
    ground_truth = jieba.lcut(s)  # 'ground truth'
    fmm = FMM(s, wordlist, maxlen)
    bmm = BMM(s, wordlist, maxlen)

    print('jieba:     ', &quot;/&quot;.join(ground_truth))

    print('-' * 20)

    print('FMM:       ', &quot;/&quot;.join(fmm))
    hits, len_truth, len_result, _ = calc_hits(ground_truth, fmm)
    P, R, F = calc_PRF(hits, len_truth, len_result)
    print('Hits:      ', hits)
    print('Precision: ', P)
    print('Recall:    ', R)
    print('F:         ', F)

    print('-' * 20)

    print('BMM:       ', &quot;/&quot;.join(bmm))
    hits, len_truth, len_result, _ = calc_hits(ground_truth, bmm)
    P, R, F = calc_PRF(hits, len_truth, len_result)
    print('Hits:      ', hits)
    print('Precision: ', P)
    print('Recall:    ', R)
    print('F:         ', F)
</code></pre>
<h3 id="训练及测试"><a class="header" href="#训练及测试">训练及测试</a></h3>
<p>打印统计信息帮助函数：</p>
<p><a href="lab/02/./print_helper.py">print_helper.py</a></p>
<pre><code class="language-python">from numpy.core.numeric import NaN
from calc import calc_PRF

def print_stat(foo_name, total_truth_cnt, total_result_cnt, total_hits):

    print('{} 分词结果：'.format(foo_name))
    print(&quot;{:&lt;8} 分词总共的数目：{}&quot;.format(&quot;jieba&quot;, total_truth_cnt))
    print(&quot;{:&lt;8} 分词总共的数目：{}&quot;.format(foo_name, total_result_cnt))
    print(&quot;{:&lt;8} 分词正确的数目：{}&quot;.format(foo_name, total_hits))

    P, R, F = NaN, NaN, NaN

    if total_result_cnt != 0 and total_truth_cnt != 0:
        P, R, F = calc_PRF(total_hits, total_truth_cnt, total_result_cnt)

    print(&quot;准确率（P）：{:.5f} %&quot;.format(100 * P))
    print(&quot;回归率（R）：{:.5f} %&quot;.format(100 * R))
    print(&quot;F 值为：{}&quot;.format(F))
</code></pre>
<p>对文档进行分词尝试，使用 jieba 分词以及 FMM、BMM 两种最大匹配算法进行分词，并评估 FMM、BMM 分词的结果。</p>
<p>同时，根据与 jieba 分词结果的差异得到“新词”并添加进词典。最后，将得到的新词典写出到 <code>new_wordlist.dic</code> 文件：</p>
<p><a href="lab/02/./train.py">train.py</a></p>
<pre><code class="language-python">import jieba
from FMM_BMM_trie import *   # 根据路径不同修改
from calc import *
from file_processing import * 

trainning_file_path = [&quot;训练语料&quot;]

wordlist, cnt, maxlen = load_wordlist('wordlist.dic')

FMM_cut = lambda line : FMM(line, wordlist, maxlen)
BMM_cut = lambda line : BMM(line, wordlist, maxlen)

methods = [
    ('jieba', jieba.lcut),
    ('FMM', FMM_cut),
    ('BMM', BMM_cut)
]

tot_hits = {'FMM': 0, 'BMM': 0}         # 统计 FMM/BMM 分词结果正确的个数
tot_result_cnt = {'FMM': 0, 'BMM': 0}   # 统计 FMM/BMM 分词结果的个数
tot_truth_cnt = 0                       # 统计 jieba 分词结果的个数

new_words = []

tot_elapsed_time = 0

for path in trainning_file_path:
    # 对某一目录下结果进行处理
    for filename, results, elapsed_time in process_path(path, methods):
        
        if len(results) == 0: continue
        
        truth = results['jieba']
        fmm = results['FMM']
        bmm = results['BMM']

        hits, len_truth, len_result, missmatches = calc_hits(truth, fmm)
        tot_hits['FMM'] += hits
        tot_result_cnt['FMM'] += len_result
        new_words += missmatches

        hits, len_truth, len_result, missmatches = calc_hits(truth, bmm)
        tot_hits['BMM'] += hits
        tot_result_cnt['BMM'] += len_result
        new_words += missmatches
        
        tot_truth_cnt += len_truth

        tot_elapsed_time += elapsed_time
        
        # 对同一个文档进行的 FMM 和 BMM 处理，虽然存在差异
        # 但大部分分词结果相同，故只将 FMM 分词结果存储

        # write_results(filename, {'FMM': results[&quot;FMM&quot;]}, '/')

print('总耗时：{} s'.format(tot_elapsed_time))

import print_helper as helper

# 打印统计信息

helper.print_stat('FMM', tot_truth_cnt, tot_result_cnt['FMM'], tot_hits['FMM'])
helper.print_stat('BMM', tot_truth_cnt, tot_result_cnt['BMM'], tot_hits['BMM'])

# 将新词添加入词典
wordlist, cnt, maxlen = add_words(wordlist, cnt, maxlen, new_words)

save_wordlist('new_wordlist.dic', wordlist)
</code></pre>
<p>使用训练后得到的词典，进行 FMM、BMM 分词，并对结果评估（亦以 jieba 分词的结果为标准答案）：</p>
<p><a href="lab/02/./test.py">test.py</a></p>
<pre><code class="language-python">import jieba
from FMM_BMM_trie import *   # 根据路径不同修改
from calc import *
from file_processing import * 

wordlist, cnt, maxlen = load_wordlist('new_wordlist.Dic')

FMM_cut = lambda line : FMM(line, wordlist, maxlen)
BMM_cut = lambda line : BMM(line, wordlist, maxlen)

methods = [
    ('jieba', jieba.lcut),
    ('FMM', FMM_cut),
    ('BMM', BMM_cut)
]

test_file_path = [&quot;测试语料&quot;]

tot_hits = {'FMM': 0, 'BMM': 0}
tot_result_cnt = {'FMM': 0, 'BMM': 0}
tot_truth_cnt = 0

tot_elapsed_time = 0

for path in test_file_path:
    # 对某一目录下结果进行处理
    for filename, results, elapsed_time in process_path(path, methods):
        
        if len(results) == 0: continue

        truth = results['jieba']
        fmm = results['FMM']
        bmm = results['BMM']

        hits, len_truth, len_result, _ = calc_hits(truth, fmm)
        tot_hits['FMM'] += hits
        tot_result_cnt['FMM'] += len_result

        hits, len_truth, len_result, _ = calc_hits(truth, bmm)
        tot_hits['BMM'] += hits
        tot_result_cnt['BMM'] += len_result
        
        tot_truth_cnt += len_truth

        tot_elapsed_time += elapsed_time


print(&quot;------------------训练后---------------------&quot;)

print('总耗时：{} s'.format(tot_elapsed_time))

import print_helper as helper

# 打印统计信息

helper.print_stat('FMM', tot_truth_cnt, tot_result_cnt['FMM'], tot_hits['FMM'])
helper.print_stat('BMM', tot_truth_cnt, tot_result_cnt['BMM'], tot_hits['BMM'])
</code></pre>
<h3 id="trie-模块"><a class="header" href="#trie-模块">Trie 模块</a></h3>
<p>考虑到可能会涉及大量词语的存储与检索，尝试使用将词库载入并存储于自己实现的 Trie 字典树结构中。</p>
<blockquote>
<p>实际测试时，使用该 Trie 实现的效率不及使用 Python 内建的集合类型。</p>
</blockquote>
<p>为了简便起见，使用 Python 中的字典结构模拟节点对象。</p>
<p><a href="lab/02/./my_trie.py">my_trie.py</a></p>
<pre><code class="language-python">def insert(node, s):
    current = node
    for c in s:
        if c not in current:
            current[c] = dict()
        current = current[c]
    current['end'] = True


def find(node, s):
    for c in s:
        if c not in node:
            return False
        node = node[c]
    return 'end' in node


def traverse(node, s=''):
    for key in node.keys():
        if key == 'end':
            continue
        s += key
        yield from traverse(node[key], s)
        s = s[:-1]
    if 'end' in node:
        yield s

</code></pre>
<p>使用示例：<a href="lab/02/./trie_test.py">trie_test.py</a></p>
<p>使用 trie 进行词典检索的 FMM、BMM：<a href="lab/02/./FMM_BMM_trie.py">FMM_BMM_trie.py</a>。</p>
<h2 id="五实验结果-1"><a class="header" href="#五实验结果-1">五、实验结果</a></h2>
<h3 id="基本实验结果"><a class="header" href="#基本实验结果">基本实验结果</a></h3>
<p>实验采用的语料来自于知乎问答平台上针对“大学”的回答。初步实验材料的大小如下：</p>
<figure>
<img src="lab/02/./assets/train-material.png" 
    alt="用于训练的材料：2789 个文件，2.63 MB" 
    title="用于训练的材料大小"
    style="max-width: 50%;">
<figcaption>图 1：用于训练的材料（2789 个文件，2.63 MB）</figcaption>
</figure>
<figure>
<img src="lab/02/./assets/test-material.png" 
    alt="用于测试的材料：1747 个文件，1.61 MBB" 
    title="用于测试的材料大小"
    style="max-width: 50%;">
<figcaption>图 2：用于测试的材料（1747 个文件，1.61 MB）</figcaption>
</figure>
<p>首先使用默认字典（53143 词），对训练材料进行分词，并对结果评估，最后产生新的字典。（使用自己编写的 Trie 作为存储词典的结构）</p>
<p>代码：<a href="lab/02/./train.py">train.py</a></p>
<figure>
<p><img src="lab/02/./assets/trie-before-train.png" alt="使用默认词典的情况下，FMM、BMM 法分词的结果" /></p>
<figcaption>图 3：使用默认词典的情况下，FMM、BMM 法分词的结果</figcaption>
</figure>
<pre><code>总耗时：8.89 s

FMM 分词结果：
jieba    分词总共的数目：70222
FMM      分词总共的数目：82647
FMM      分词正确的数目：47265
准确率（P）：57.18901 %
回归率（R）：67.30797 %
F 值为：0.6183725935277917

BMM 分词结果：
jieba    分词总共的数目：70222
BMM      分词总共的数目：82643
BMM      分词正确的数目：47414
准确率（P）：57.37207 %
回归率（R）：67.52015 %
F 值为：0.6203382069145978
</code></pre>
<p>得到新的词典拥有 57692 条词。接着，使用学习“新词”后的词典，对其余的资料进行分词，并评估分词结果。（使用自己编写的 Trie 作为存储词典的结构）</p>
<p>代码：<a href="lab/02/./test.py">test.py</a></p>
<figure>
<p><img src="lab/02/./assets/trie-after-train.png" alt="使用学习“新词”后的词典，FMM、BMM 法分词的结果" /></p>
<figcaption>图 4：使用学习“新词”后的词典，FMM、BMM 法分词的结果</figcaption>
</figure>
<pre><code>总耗时：21.36 s
FMM 分词结果：
jieba    分词总共的数目：51258
FMM      分词总共的数目：55242
FMM      分词正确的数目：38370
准确率（P）：69.45802 %
回归率（R）：74.85661 %
F 值为：0.7205633802816901
BMM 分词结果：
jieba    分词总共的数目：51258
BMM      分词总共的数目：55252
BMM      分词正确的数目：38563
准确率（P）：69.79476 %
回归率（R）：75.23313 %
F 值为：0.7241198009576565
</code></pre>
<p>可以看到，虽然词典扩增规模不明显，但耗时显著增加。</p>
<h3 id="词典容量的影响"><a class="header" href="#词典容量的影响">词典容量的影响</a></h3>
<p>很显然，对于词典匹配法，当词典的容量越大，效果越好，进行分词所需的时间越多。</p>
<p>经过多次测试发现，在使用初始词典进行分词的情况下，正确率在 60% 左右。随着词典的中单词数量扩充至 17 ~ 18 万左右时，正确率可以达到 84% ~ 86%。</p>
<p>随着词典数量的进一步提升，分词的正确率的提升较小，但时间方面的开销却越来越大。</p>
<h2 id="六总结-1"><a class="header" href="#六总结-1">六、总结</a></h2>
<p>在这次实验中，练习了使用 FMM、BMM 以及 jieba 分词库对文本进行分词处理，在整个的实验过程中，通过查询相关资料等途径解决许多问题，实践能力得到了较大的提升。</p>
<p>仍存在正确分词个数统计不准确的问题，有待完善。</p>
<h3 id="关于如何计算正确匹配数"><a class="header" href="#关于如何计算正确匹配数">关于如何计算正确匹配数</a></h3>
<p>简单的方法是，</p>
<pre><code class="language-python">hits += len([i for i in result if i in truth]) 
miss += [i for i in result if i not in truth]
</code></pre>
<p>使用这种方法，当一句话中出现多个相同词汇时，可能会对结果造成影响。可以根据逗号（，）等符号将语句切分为不同的小段，并对每一个小段进行处理，可以较大程度上减少相同词汇出现的概率，一定程度上保证统计的正确率。</p>
<p>本次实验采用了如 <a href="lab/02/./calc.py">calc.py</a> 中 <code>calc_hits</code> 方法的实现。</p>
<h2 id="参考资料"><a class="header" href="#参考资料">参考资料</a></h2>
<ul>
<li><a href="https://blog.csdn.net/u013510838/article/details/81673016">自然语言处理 1：分词 - CSDN</a></li>
<li><a href="https://blog.csdn.net/u013510838/article/details/81738431">自然语言处理 2：jieba 分词用法及原理 - CSDN</a></li>
<li><a href="https://github.com/fxsjy/jieba">fxsjy/jieba - GitHub</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/100552669">NLP 中文分词的评估指标 - 知乎</a></li>
<li><a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural language processing - Wikipedia</a></li>
<li><a href="https://stackoverflow.com/questions/11015320/how-to-create-a-trie-in-python/11015381">https://stackoverflow.com/questions/11015320/how-to-create-a-trie-in-python/11015381</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="自然语言处理实验-1"><a class="header" href="#自然语言处理实验-1">自然语言处理实验</a></h1>
<p>实验一</p>
<table>
    <style>
        th { text-justify: distribute; text-align-last: justify; text-align: justify; text-align: center; }
        td { text-align: center; }
    </style>
    <tbody>
        <tr><th>学院</th><td>信息工程学院</td></tr>
        <tr><th>指导教师</th><td>孙媛</td></tr>
        <tr><th>班级</th><td>19 级计算机科学与技术 1 班</td></tr>
        <tr><th>学生姓名</th><td>John Doe</td></tr>
        <tr><th>学号</th><td>19000000</td></tr>
    </tbody>
</table>
<p>日期： 2021 年 9 月 29 日</p>
<h2 id="摘要-1"><a class="header" href="#摘要-1">摘要</a></h2>
<h2 id="目录-2"><a class="header" href="#目录-2">目录</a></h2>
<h2 id="一实验内容-2"><a class="header" href="#一实验内容-2">一、实验内容</a></h2>
<p>本次实验主要做什么？</p>
<h2 id="二实验原理-2"><a class="header" href="#二实验原理-2">二、实验原理</a></h2>
<p>包含：存在的主要问题，用什么方法解决，原理是什么？</p>
<h2 id="三整体框架-2"><a class="header" href="#三整体框架-2">三、整体框架</a></h2>
<p>包含整体框图，各主要模块的功能。</p>
<p>图表都需要 <strong>带编号</strong></p>
<h2 id="四主要程序模块-2"><a class="header" href="#四主要程序模块-2">四、主要程序模块</a></h2>
<p>详细介绍各个主要模块的功能及实现流程。</p>
<h2 id="五实验结果-2"><a class="header" href="#五实验结果-2">五、实验结果</a></h2>
<p>详细分析实验结果，除了包含定量评价，还要有定性评价。
对存在的问题，要着重剖析。</p>
<h2 id="六总结-2"><a class="header" href="#六总结-2">六、总结</a></h2>
<p>除了对整个实验进行概要总结，如果有程序亮点，可以在这阐述。</p>
<h2 id="参考文献-1"><a class="header" href="#参考文献-1">参考文献</a></h2>
<p>如有参考文献，请附上。</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
    </body>
</html>
