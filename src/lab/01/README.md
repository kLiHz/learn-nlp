# 自然语言处理实验：实验一

编写程序自动抓取多网页内容

{{#include ../author-info.html}}

日期： 2021 年 9 月 29 日

## 摘要


## 目录

## 一、实验内容

学会使用 Python（或其他编程语言）编写网络爬虫程序，能对网站格式进行解析，爬取相关内容并过滤，建立语料库。

要求：能够自动爬取多个网页，过滤掉图片、视频信息，存取文中中的 `title`，`context`，`URL` 等关键文本字段，存取到不同的文本和目录中。

提交内容：

- 实验报告
- 程序代码
- 爬取的语料

本次实验选取了问答平台 [知乎][zhihu] 作为实验对象。作为一个问答平台，知乎网站上内容的基本组成为问题，以及若干相应的答案；每个问题及问题下的回答均具有唯一的数字 URL 与之对应；此外，[知乎][zhihu] 存在话题的概念，每个话题下可以包含一系列问题。

[zhihu]: https://zhihu.com

## 二、实验原理

互联网上的网页通过超链接建立起彼此之间的联系。对于一个页面，通过检索其上存在的链接，并逐个访问这些链接、重复上述操作，就能够遍历该页面所在的整个网络。

传统的互联网页面的大多数元素通常包含在 HTML 文档源代码中，在了解了目标网站的 HTML 页面结构后，就能够如同在树上摘果子一般，从中提取自己需要的内容。

不过，现代网站多大量采用动态网页技术，以便更及时、更具针对性的向用户呈现内容，抑或作为一种反制网页爬取的对策。即，程序获取到的 HTML 源代码结构，并不是用户在浏览器中实际所看到的样子。一般来说，社交、购物网站经常使用这样的技术。若需从动态网站上获得信息，最理想的办法是使用网站方提供的应用程序接口；当这样的接口不存在时，则只能通过应用程序模拟真实用户使用浏览器时的操作，从而获得到页面上的元素。

除此之外，许多网站也会要求事先用户进行登录，之后才会提供更多信息。网站一般通过 Cookie 技术识别用户，因此可以通过在程序中使用正常登录后得到的 Cookie 来进行模拟登录。

## 三、整体框架

实验目的是获取语料。在实验开始前，要先决定目标网站上感兴趣的信息，以及存储时使用的结构。对于本次实验的对象——知乎问答社区，感兴趣的信息主要是其上的问题以及相应的答案。不过为了使获取得到的内容更具相关性，选择一个话题作为研究对象更为经济。

根据前文所述，知乎问答社区的问题具有唯一的 URL，每个问题（或问题下的某个回答）可以视作信息抓取过程中最核心的着手对象。

截止实验时，进入知乎社区的话题页，即可以得到关于话题的简介，以及话题下的热门问题。

对于某一个话题下信息的存储，设计了如下的目录结构：

```
TOPIC_ID
│
├── URL
├── DETAILS
├── SUMMARY
│
├── QUESTIONS
│   │
│   ├── QUESTION_ID_1
│   │   ├── ANSWER_ID_1
│   │   │   └── ANSWER_DETAILS
│   │   ├── ANSWER_ID_2
│   │   │   └── ANSWER_DETAILS
│   │   ....
│   │
│   ├── QUESTION_ID_2
│   │   ├── ANSWER_ID_1
│   │   │   └── ANSWER_DETAILS
│   │   ├── ANSWER_ID_2
│   │   │   └── ANSWER_DETAILS
│   │   ....
│
....
```

> 包含整体框图，各主要模块的功能。
>
> 图表都需要带编号

## 四、主要程序模块

详细介绍各个主要模块的功能及实现流程。

## 五、实验结果

详细分析实验结果，除了包含定量评价，还要有定性评价。
对存在的问题，要着重剖析。

## 六、总结

除了对整个实验进行概要总结，如果有程序亮点，可以在这阐述。

## 参考文献

- <https://ithelp.ithome.com.tw/articles/10225429>
- <https://marcovaldong.github.io/2016/08/18/Python爬虫爬取知乎小结/>
